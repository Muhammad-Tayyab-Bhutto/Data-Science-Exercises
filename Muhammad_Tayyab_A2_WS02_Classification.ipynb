{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammad-Tayyab-Bhutto/Data-Science-Exercises/blob/main/Muhammad_Tayyab_A2_WS02_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtroMAMDkiOn"
      },
      "source": [
        "## Artificial Intelligence Practical\n",
        "## WorkSheet 02 (Classification)\n",
        "This worksheet is intented to give you clear idea of Classification and its practical implementation in python.\n",
        "You are required to complete all tasks and submit this worksheet with all required datasets that you have used.\n",
        "\n",
        "You can take help from online for description and implementation of your Classification concepts. You are also provided with a folder containing enough support materials to complete following tasks.\n",
        "\n",
        "You all conceptual description and implementation should be on this worksheet notebook.\n",
        "\n",
        "\n",
        "Task 03:\n",
        "\n",
        "* What is Logistic Regression? Is it used for Regression or Classification? Is it used for binary and multi-classification problems? Implement Logistic regression for binary and also describe its various parameters of its model.\n",
        "\n",
        "\n",
        "Task 04:\n",
        "* What is Linear Discreminant Analysis (LDA)? Implement LDA for binary and multi-classification.\n",
        "\n",
        "Task 05:\n",
        "\n",
        "* How you would evaluated the performance of different classification models?\n",
        "Please, list and describe performance metrics for Classification.\n",
        "* Also, Compare the performance of atleast three classification models on same dataset for binary and multi-classification.\n",
        "\n",
        "Task 06: Optional\n",
        "* You can also, fine tune different parameters of various models to get best results for the classification.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 01:\n",
        "\n",
        "\n",
        "*   What is Classification in Machine Learning, its types.\n",
        "\n",
        "Classification is a supervised machine learning method where the model tries to predict the correct label of a given input data. In classification, the model is fully trained using the training data, and then it is evaluated on test data before being used to perform prediction on new unseen data.\n",
        "\n",
        "Type of Classification:\n",
        "\n",
        "Binary Classification\n",
        "\n",
        "In a binary classification task, the goal is to classify the input data into two mutually exclusive categories. The training data in such a situation is labeled in a binary format: true and false; positive and negative; O and 1; spam and not spam, etc. depending on the problem being tackled. For instance, we might want to detect whether a given image is a truck or a boat.\n",
        "\n",
        "Multi-Class Classification\n",
        "\n",
        "The multi-class classification, on the other hand, has at least two mutually exclusive class labels, where the goal is to predict to which class a given input example belongs to. In the following case, the model correctly classified the image to be a plane.\n",
        "\n",
        "Multi-Label Classification\n",
        "\n",
        "In multi-label classification tasks, we try to predict 0 or more classes for each input example. In this case, there is no mutual exclusion because the input example can have more than one label.\n",
        "\n",
        "Such a scenario can be observed in different domains, such as auto-tagging in Natural Language Processing, where a given text can contain multiple topics. Similarly to computer vision, an image can contain multiple objects, as illustrated below: the model predicted that the image contains: a plane, a boat, a truck, and a dog."
      ],
      "metadata": {
        "id": "T1QvpO0kUbgn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 02:\n",
        "# What is binary and multi-classification? What is  Naive Bayes Classifier? Implement NB classifier with binary and multi-classification.\n",
        "# Naive Bayes Classifier\n",
        " Naive Bayes classification is a straightforward and powerful algorithm for the classification task. Naive Bayes classification is based on applying Bayes’ theorem with strong independence assumption between the features. Naive Bayes classification produces good results when we use it for textual data analysis such as Natural Language Processing.\n",
        " Naive Bayes Classifier assumes that all the features are unrelated to each other. Presence or absence of a feature does not influence the presence or absence of any other feature.\n",
        " In real world datasets, we test a hypothesis given multiple evidence on features. So, the calculations become quite complicated. To simplify the work, the feature independence approach is used to uncouple multiple evidence and treat each as an independent one.   \n",
        "\n",
        " Binary Classification:\n",
        "  It is the task of categorizing items into one of two classes or categories.\n",
        "  For example, spam detection (spam or not spam), sentiment analysis (positive or negative sentiment), medical diagnosis (disease present or not present), etc.\n",
        "\n",
        " Multi-class Classification:\n",
        "  It is the task of categorizing items into one of three or more classes or categories.\n",
        "  For example, image recognition (identifying different types of animals), language identification (detecting which language a given text belongs to), etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "vv9Fb55sRHwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps:\n",
        "first I have import dummy data from sklearn dataset then I have used sklearn train_test_split function which will split data for test and then I have imported the naive bayes lastly I have measured accuracy score using sklearn accuracy_score function"
      ],
      "metadata": {
        "id": "Ckb9GalPTbG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris;\n",
        "from sklearn.model_selection import train_test_split;\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB;\n",
        "from sklearn.metrics import accuracy_score;"
      ],
      "metadata": {
        "id": "auLfSPnizqah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steps for solving problem\n",
        "* load the data\n",
        "* now assign the data dependent variable and independent variable (X, y)\n",
        "* Assume a binary classification task, class 0 and the rest as class 1\n",
        "* Now split data into training and testing sets\n",
        "* Create and train the Naive Bayes classifier\n",
        "* Make predictions\n",
        "* Calculate accuracy\n"
      ],
      "metadata": {
        "id": "QXXw3JZKTo8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Assume a binary classification task, class 0 and the rest as class 1 it means assume 0 to that class which we want to classifie\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Binary Classification Accuracy:\", accuracy)\n",
        "\n",
        "\n",
        "# Multi-class Classification Example\n",
        "# Load data\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Naive Bayes classifier\n",
        "nb_classifier_multi = MultinomialNB()\n",
        "nb_classifier_multi.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_multi = nb_classifier_multi.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_multi = accuracy_score(y_test, y_pred_multi)\n",
        "print(\"Multi-class Classification Accuracy:\", accuracy_multi)"
      ],
      "metadata": {
        "id": "QRBPPdgQTXjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Logistic Regression?\n",
        "\n",
        "Logistic Regression is a statistical method used for classification tasks. Despite its name, it is not used for regression (predicting continuous values), but rather for predicting the probability of an instance belonging to a particular class. It’s commonly used in binary classification (two classes) and can be extended to handle multi-class problems.\n",
        "\n",
        "Binary vs. Multi-Classification:\n",
        "\n",
        "Binary Classification:\n",
        "\n",
        "In binary classification, we have two possible classes (e.g., spam or not spam, yes or no).\n",
        "\n",
        "Logistic Regression models the probability that an instance belongs to the positive class.\n",
        "\n",
        "The output is a probability score between 0 and 1.\n",
        "\n",
        "A threshold (usually 0.5) is used to classify instances as positive or negative.\n",
        "\n",
        "Multi-Class Classification:\n",
        "\n",
        "\n",
        "In multi-class classification, there are more than two classes (e.g., classifying animals into cat, dog, or bird).\n",
        "\n",
        "Logistic Regression can be extended to handle multi-class problems using one of the following approaches:\n",
        "One-vs-Rest (OvR): Train a separate binary classifier for each class, treating it as the positive class and the rest as the negative class.\n",
        "\n",
        "Multinomial (Softmax): Use a single model with multiple output classes, where the predicted probabilities sum to 1.\n"
      ],
      "metadata": {
        "id": "kivyc6UCbAzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Logistic Regression (Binary)"
      ],
      "metadata": {
        "id": "jXbaRcGbbSJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(solver='lbfgs', max_iter=100)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHFgVqYzbVsf",
        "outputId": "b7cc283e-7318-4e56-de21-8e3df013b793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.96, Precision: 0.96, Recall: 0.99, F1-score: 0.97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgb2x8zkFZL9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 04:\n",
        "\n",
        "Linear Discriminant Analysis (LDA):\n",
        "\n",
        "Linear Discriminant Analysis (LDA) is another classification technique that aims to find a linear combination of features that best separates different classes. It is commonly used for dimensionality reduction and classification."
      ],
      "metadata": {
        "id": "8BOdMEcIcNO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 05: Evaluating Classification Models\n",
        "\n",
        "To evaluate classification models, consider the following performance metrics:\n",
        "\n",
        "Accuracy: Overall correctness of predictions.\n",
        "\n",
        "Precision: Proportion of true positive predictions among all positive predictions.\n",
        "\n",
        "Recall (Sensitivity): Proportion of true positive predictions among all actual positive instances.\n",
        "\n",
        "F1-score: Harmonic mean of precision and recall.\n",
        "\n",
        "Comparing models on the same dataset involves training different classifiers (e.g., Logistic Regression, Decision Tree, Random Forest) and evaluating their performance using the metrics above."
      ],
      "metadata": {
        "id": "i1rXLMaWddUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 06: (Optional)\n",
        "\n",
        "Fine-tuning parameters of various models involves techniques like Grid Search or Random Search to find the best combination of hyperparameters for optimal performance. This can be done using tools like scikit-learn's GridSearchCV or RandomizedSearchCV."
      ],
      "metadata": {
        "id": "4wWeeRhQe4MK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}